{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#3.-Principal-Component-Analysis\" data-toc-modified-id=\"3.-Principal-Component-Analysis-1\">3. Principal Component Analysis</a></span></li><li><span><a href=\"#9.-Bayes-and-KNN-Classifier\" data-toc-modified-id=\"9.-Bayes-and-KNN-Classifier-2\">9. Bayes and KNN Classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#9.1.-Bayes\" data-toc-modified-id=\"9.1.-Bayes-2.1\">9.1. Bayes</a></span></li><li><span><a href=\"#9.2.-KNN\" data-toc-modified-id=\"9.2.-KNN-2.2\">9.2. KNN</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(503)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data matrix\n",
    "X = np.array([\n",
    "    [1, 0, 0.5],\n",
    "    [6, 14, 3],\n",
    "    [11, 28, 5.5],\n",
    "    [7, 21, 3.5]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.47391105, -1.52127766, -1.47391105],\n",
       "       [-0.07018624, -0.16903085, -0.07018624],\n",
       "       [ 1.33353857,  1.18321596,  1.33353857],\n",
       "       [ 0.21055872,  0.50709255,  0.21055872]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize for PCA\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(X_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.57834475 -0.57535615 -0.57834475]]\n"
     ]
    }
   ],
   "source": [
    "# First principal direction\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.5801339   0.17843663 -2.22326064 -0.53530988]]\n"
     ]
    }
   ],
   "source": [
    "# First principal components\n",
    "print(pca.components_ @ X_standardized.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00680207]\n"
     ]
    }
   ],
   "source": [
    "# Remaining variance\n",
    "print(1 - pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Bayes and KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(503)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., 95., ...,  0.,  0.,  0.],\n",
       "       [ 0., 78., 95., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(\"data/usps-2cls.csv\", delimiter=\",\")\n",
    "data[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = (2200, 256) matrix:\n",
      "[[ 0.  0. 95. ...  0.  0.  0.]\n",
      " [ 0. 78. 95. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\n",
      "y = (2200,) vector:\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Designate feature matrix X, response vector y\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "print(f\"X = {X.shape} matrix:\\n{X[:5, :]}\\n\")\n",
    "print(f\"y = {y.shape} vector:\\n{y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_means(X, y):\n",
    "    \"\"\"Calculates mean vectors for each class c found in the response vector y.\"\"\"\n",
    "    mus = []\n",
    "    for c in np.sort(np.unique(y)):\n",
    "        mus.append(np.mean(X[(y == c), :], axis=0))\n",
    "    \n",
    "    return mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_covs(X, y):\n",
    "    \"\"\"Calculates covariance matrices for each class c found in the response vector y.\"\"\"\n",
    "    Sigmas = []\n",
    "    for c in np.sort(np.unique(y)):\n",
    "        # rowvar: data are rows; bias: calculate sample cov (normalize by N-1)\n",
    "        Sigmas.append(np.cov(X[(y == c), :], rowvar=False, bias=False))\n",
    "    \n",
    "    return Sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notes on simplification\n",
    "\n",
    "The function below uses the faster (and more stable) method of calculating probabilities under a multivariate Gaussian distribution with high dimensionality. This method was documented earlier in this course and gets around numerical issues that occur when attempting to invert the covariance matrix (see `speed_up_GMM.pdf`). To recap the method:\n",
    "\n",
    "1. Compute eigendecomposition of $\\Sigma = U \\Lambda U^\\top$\n",
    "2. Approximate $\\tilde{\\Sigma}$ (the low-rank approximation of $\\Sigma$) $ = \\tilde{U}\\tilde{\\Lambda}\\tilde{U}$ where $\\tilde{U}$ is the matrix of the $r$ eigenvectors associated with the $r$ largest eigenvalues of $\\Sigma$ and $\\tilde{\\Lambda}$\n",
    "3. Compute $\\tilde{x} = \\tilde{U}^\\top x$, the low-rank approximation of the dataset\n",
    "4. Compute $\\tilde{mu} = \\tilde{U}^\\top \\mu$, the low-rank approximation of the mean vector for the distribution\n",
    "\n",
    "With those approximations, calculate density:\n",
    "\n",
    "$$ \\mathcal{N}(x; \\mu, \\Sigma) \\approx \\frac{1}{\\sqrt{(2\\pi)^d \\prod_{i=1}^{r} \\lambda_i}} exp\\left\\{ -\\frac{1}{2} \\sum_{i=1}^{r} \\frac{(\\tilde{x}_i - \\tilde{\\mu}_i)^2}{\\lambda_i} \\right\\} $$\n",
    "\n",
    "In the formula above, $det(\\Sigma)$ is replaced by $det(\\tilde{\\Sigma}) = \\prod_{i=1}^{r} \\lambda_i$.\n",
    "\n",
    "The formula can also be written as:\n",
    "$$ \\mathcal{N}(x; \\mu, \\Sigma) \\approx \\frac{exp\\left\\{ -\\frac{1}{2} \\sum_{i=1}^{r} \\frac{(\\tilde{x}_i - \\tilde{\\mu}_i)^2}{\\lambda_i} \\right\\}}{\\sqrt{(2\\pi)^d \\prod_{i=1}^{r} \\lambda_i}} \\approx \\frac{exp\\left\\{ -\\frac{1}{2} \\sum_{i=1}^{r} \\frac{(\\tilde{x}_i - \\tilde{\\mu}_i)^2}{\\lambda_i} \\right\\}}{\\sqrt{(2\\pi)^d} \\cdot \\sqrt{\\prod_{i=1}^{r} \\lambda_i}} $$\n",
    "\n",
    "Since $\\sqrt{(2\\pi)^d}$ is just a constant, we can eliminate that - it does not matter for the purpose of comparing likelihoods, as the constant is the same for both distributions. This means we only need to calculate $\\sqrt{\\prod_{i=1}^r \\lambda_i}$. Omitting the constant results in:\n",
    "$$ \\ell(x, \\mu, \\Sigma) \\approx \\mathcal{N}(x; \\mu, \\Sigma) \\approx \\frac{exp\\left\\{ -\\frac{1}{2} \\sum_{i=1}^{r} \\frac{(\\tilde{x}_i - \\tilde{\\mu}_i)^2}{\\lambda_i} \\right\\}}{\\sqrt{\\prod_{i=1}^{r} \\lambda_i}} $$\n",
    "\n",
    "In addition, the function serves the same purpose as calculating $p(x_i|y)$ for use with Bayes' theorem: $p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$.\n",
    "\n",
    "However, we only need the $p(x|y)$ part of Bayes' theorem for this problem because:\n",
    "1. $p(x)$ is constant between the two classes (as a marginal probability)\n",
    "2. $p(y = 0) = p(y = 1) = 0.5$\n",
    "\n",
    "In addition, as noted above, the actual calculation used in the function below will not return the true probability, because the true probability is not needed for selecting a class - only relative likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood(X, mu, Sigma, r=None):\n",
    "    \"\"\"Calculates likelihood under a multivariate normal distribution.\n",
    "    \n",
    "    Some constant/scaling terms are left out for simplicity, so this is not true probability, but it serves the purpose\n",
    "    of determining relative p(x|y) given a class y, a mean vector mu of features for data points in class y, and a covariance\n",
    "    matrix Sigma. This assumes a Gaussian/normal distribution of data for each feature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        A data matrix in row format (features as column vectors) with shape (m, n).\n",
    "    Sigma : numpy array\n",
    "        A covariance matrix with shape (n, n).\n",
    "    mu : numpy array\n",
    "        A mean vector with shape (n, ).\n",
    "    r : int\n",
    "        A rank r for approximating Sigma in order to efficiently calculate probabilities under a Gaussian\n",
    "        distribution using the sped-up method.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    density : numpy array\n",
    "        An array of probabilities with shape (m, ).       \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    if r is None:\n",
    "        r = n\n",
    "    \n",
    "    # Eigendecomposition of Sigma, taking only the top r eigenpairs\n",
    "    lam_approx, U_approx = linalg.eigh(Sigma, eigvals=(n-r, n-1))\n",
    "    \n",
    "    # Approximate the data and means\n",
    "    X_approx = (U_approx.T @ X.T).T  # Approximation of the input data\n",
    "    mu_approx = U_approx.T @ mu.reshape(-1, 1)\n",
    "    \n",
    "    # Numerator of density function (passed to exp as needed)\n",
    "    normalized_distances_sq = -0.5 * np.sum(((X_approx - mu_approx.T)**2) / lam_approx, axis=1)\n",
    "    \n",
    "    # Denominator of density function with constant removed\n",
    "    lam_product_sq = np.sqrt(np.product(lam_approx))\n",
    "    \n",
    "    # Calculate density using rank-r approximations\n",
    "    density = np.exp(normalized_distances_sq) / lam_product_sq\n",
    "    \n",
    "    return density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_misclassification(X, y, mus, Sigmas, r):\n",
    "    \"\"\"Calculates misclassification rate given multiple a list of mean vectors (mus) and covariance matrices (Sigmas).\"\"\"\n",
    "    \n",
    "    # Calculate likelihood under each class' multivariate Gaussian distribution\n",
    "    likelihoods = []\n",
    "    for mu, Sigma in zip(mus, Sigmas):\n",
    "        likelihoods.append(calculate_likelihood(X, mu, Sigma, r=r))\n",
    "    \n",
    "    # Pick a class: under which distribution does x_i have a greater likelihood?\n",
    "    likelihoods = np.array(likelihoods).T    \n",
    "    predicted_y = np.argmax(likelihoods, axis=1)\n",
    "    \n",
    "    # Compare selected classes to actual labels\n",
    "    error = np.where(y != predicted_y)[0].shape[0] / y.shape[0]\n",
    "    \n",
    "    return likelihoods, predicted_y, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating p=0.1\n",
      "Evaluating p=0.2\n",
      "Evaluating p=0.5\n",
      "Evaluating p=0.8\n",
      "Evaluating p=0.9\n"
     ]
    }
   ],
   "source": [
    "training_errors = {}\n",
    "test_errors = {}\n",
    "\n",
    "for p in [0.1, 0.2, 0.5, 0.8, 0.9]:\n",
    "    print(f\"Evaluating p={p}\")\n",
    "    training_error = []\n",
    "    test_error = []\n",
    "    for iteration in range(100):        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=p)\n",
    "        mus = calculate_means(X_train, y_train)  # Estimate mean vectors from training data\n",
    "        Sigmas = calculate_covs(X_train, y_train)   # Estimate covariance matrices from training data\n",
    "\n",
    "        # Calculate misclassification using learned parameters\n",
    "        # r=50 was determined empirically to be a good value (from my experience with Homework 4)\n",
    "        training_results = calculate_misclassification(X_train, y_train, mus, Sigmas, r=50)\n",
    "        test_results = calculate_misclassification(X_test, y_test, mus, Sigmas, r=50)\n",
    "        \n",
    "        training_error.append(training_results[2])\n",
    "        test_error.append(test_results[2])\n",
    "        \n",
    "    training_errors[p] = np.mean(training_error)\n",
    "    test_errors[p] = np.mean(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training errors:\n",
      "{0.1: 0.09836363636363638,\n",
      " 0.2: 0.09425000000000003,\n",
      " 0.5: 0.0753909090909091,\n",
      " 0.8: 0.06661363636363637,\n",
      " 0.9: 0.06587878787878787}\n",
      "\n",
      "Test errors:\n",
      "{0.1: 0.12046969696969696,\n",
      " 0.2: 0.0872784090909091,\n",
      " 0.5: 0.0689181818181818,\n",
      " 0.8: 0.06409090909090909,\n",
      " 0.9: 0.061090909090909085}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "print(\"Training errors:\")\n",
    "pprint.pprint(training_errors)\n",
    "print(\"\\nTest errors:\")\n",
    "pprint.pprint(test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "np.random.seed(503)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating K=5\n",
      "  Evaluating p=0.1\n",
      "  Evaluating p=0.2\n",
      "  Evaluating p=0.5\n",
      "  Evaluating p=0.8\n",
      "  Evaluating p=0.9\n",
      "Evaluating K=10\n",
      "  Evaluating p=0.1\n",
      "  Evaluating p=0.2\n",
      "  Evaluating p=0.5\n",
      "  Evaluating p=0.8\n",
      "  Evaluating p=0.9\n",
      "Evaluating K=15\n",
      "  Evaluating p=0.1\n",
      "  Evaluating p=0.2\n",
      "  Evaluating p=0.5\n",
      "  Evaluating p=0.8\n",
      "  Evaluating p=0.9\n",
      "Evaluating K=30\n",
      "  Evaluating p=0.1\n",
      "  Evaluating p=0.2\n",
      "  Evaluating p=0.5\n",
      "  Evaluating p=0.8\n",
      "  Evaluating p=0.9\n"
     ]
    }
   ],
   "source": [
    "training_errors = {}\n",
    "test_errors = {}\n",
    "\n",
    "for K in [5, 10, 15, 30]:\n",
    "    print(f\"Evaluating K={K}\")\n",
    "    training_errors[K] = {}\n",
    "    test_errors[K] = {}\n",
    "    for p in [0.1, 0.2, 0.5, 0.8, 0.9]:\n",
    "        print(f\"  Evaluating p={p}\")\n",
    "        training_error = []\n",
    "        test_error = []\n",
    "        for i in range(100):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=p)\n",
    "            \n",
    "            knn = KNeighborsClassifier(n_neighbors=K)\n",
    "            knn.fit(X_train, y_train)\n",
    "            \n",
    "            training_predictions = knn.predict(X_train)\n",
    "            training_error.append(np.where(y_train != training_predictions)[0].shape[0] / y_train.shape[0])\n",
    "            \n",
    "            test_predictions = knn.predict(X_test)\n",
    "            test_error.append(np.where(y_test != test_predictions)[0].shape[0] / y_test.shape[0])\n",
    "            \n",
    "        training_errors[K][p] = np.mean(training_error)\n",
    "        test_errors[K][p] = np.mean(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training errors:\n",
      "{5: {0.1: 0.04945454545454546,\n",
      "     0.2: 0.03368181818181818,\n",
      "     0.5: 0.021445454545454556,\n",
      "     0.8: 0.017954545454545452,\n",
      "     0.9: 0.0171010101010101},\n",
      " 10: {0.1: 0.09731818181818182,\n",
      "      0.2: 0.06529545454545455,\n",
      "      0.5: 0.038172727272727275,\n",
      "      0.8: 0.029431818181818184,\n",
      "      0.9: 0.028424242424242428},\n",
      " 15: {0.1: 0.09313636363636364,\n",
      "      0.2: 0.0613409090909091,\n",
      "      0.5: 0.04017272727272728,\n",
      "      0.8: 0.029255681818181816,\n",
      "      0.9: 0.028106060606060614},\n",
      " 30: {0.1: 0.1390909090909091,\n",
      "      0.2: 0.10147727272727272,\n",
      "      0.5: 0.062272727272727264,\n",
      "      0.8: 0.04784090909090911,\n",
      "      0.9: 0.04576262626262626}}\n",
      "\n",
      "Test errors:\n",
      "{5: {0.1: 0.09110101010101011,\n",
      "     0.2: 0.06082954545454545,\n",
      "     0.5: 0.03566363636363636,\n",
      "     0.8: 0.031818181818181815,\n",
      "     0.9: 0.02959090909090909},\n",
      " 10: {0.1: 0.12991919191919193,\n",
      "      0.2: 0.08725000000000001,\n",
      "      0.5: 0.050245454545454545,\n",
      "      0.8: 0.036000000000000004,\n",
      "      0.9: 0.03504545454545455},\n",
      " 15: {0.1: 0.11701515151515152,\n",
      "      0.2: 0.07868750000000001,\n",
      "      0.5: 0.048781818181818176,\n",
      "      0.8: 0.03915909090909091,\n",
      "      0.9: 0.035},\n",
      " 30: {0.1: 0.1582424242424242,\n",
      "      0.2: 0.1153409090909091,\n",
      "      0.5: 0.07051818181818181,\n",
      "      0.8: 0.05245454545454547,\n",
      "      0.9: 0.05195454545454545}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Training errors:\")\n",
    "pprint.pprint(training_errors)\n",
    "print(\"\\nTest errors:\")\n",
    "pprint.pprint(test_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
